{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of fasq pair sequence alignment and filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "from Bio import Seq, SeqIO, Align, pairwise2\n",
    "import atriegc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_pair_reads(fastq1_name, fastq2_name):\n",
    "    reads1, reads2 = [], []\n",
    "    q1, q2 = [], []\n",
    "    \n",
    "    fastq1 = SeqIO.parse('fastq/hiv/'+fastq1_name, \"fastq\")\n",
    "    for record in fastq1:\n",
    "        reads1.append(record.seq)\n",
    "        q1.append(record.letter_annotations[\"phred_quality\"])\n",
    "\n",
    "    fastq2 = SeqIO.parse('fastq/hiv/'+fastq2_name, \"fastq\")\n",
    "    for record in fastq2:\n",
    "        reads2.append(record.seq)\n",
    "        q2.append(record.letter_annotations[\"phred_quality\"])\n",
    "    \n",
    "    return reads1, reads2, q1, q2\n",
    "\n",
    "def join_by_q(r1, r2, q1, q2):\n",
    "    q1, q2 = np.array(q1), np.array(q2)\n",
    "    res =  np.where(q1>q2, list(r1), list(r2))\n",
    "    qs = np.where(q1>q2, q1, q2)\n",
    "    return ''.join(res), qs\n",
    "\n",
    "def find_first_nucl(seq):\n",
    "    iA, iC, iT, iG = seq.find('A'), seq.find('C'), seq.find('T'), seq.find('G')\n",
    "    return min(iA, iC, iT, iG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_pairs(align, q1, q2):\n",
    "    \n",
    "    start_i = find_first_nucl(align.seqB)\n",
    "    if start_i==0:\n",
    "        return [], []\n",
    "    \n",
    "    # Add the first seqA\n",
    "    start_i_qa, start_i_qb = start_i, 0\n",
    "    seq, qs = align.seqA[:start_i], np.array(q1[:start_i])\n",
    "    #print(0, start_i)\n",
    "\n",
    "    while start_i < len(align.seqA)-1:\n",
    "\n",
    "        # Joining the overlap chunk before the first gap\n",
    "        gap_indA, gap_indB = align.seqA[start_i:].find('-'), align.seqB[start_i:].find('-')\n",
    "        if gap_indA == -1: gap_indA = len(align.seqA)\n",
    "        else: gap_indA += start_i\n",
    "        if gap_indB == -1: gap_indB = len(align.seqB)\n",
    "        else: gap_indB += start_i\n",
    "        last_i = min(gap_indA, gap_indB)\n",
    "        delta = last_i-start_i\n",
    "\n",
    "        sA, sB = align.seqA[start_i:last_i], align.seqB[start_i:last_i]\n",
    "        qA, qB = q1[start_i_qa:start_i_qa+delta], q2[start_i_qb:start_i_qb+delta]\n",
    "        new_s, new_qs = join_by_q(sA, sB, qA, qB)\n",
    "        seq += new_s\n",
    "        qs = np.append(qs, new_qs)\n",
    "        #print(start_i, last_i)\n",
    "        start_i = last_i+1\n",
    "\n",
    "        # Resolving the gap found\n",
    "        if gap_indA < gap_indB:\n",
    "            seq += align.seqB[start_i-1]\n",
    "            start_i_qa += delta\n",
    "            start_i_qb += delta+1\n",
    "            qs = np.append(qs, q2[start_i_qb-1])\n",
    "        elif gap_indA > gap_indB:\n",
    "            seq += align.seqA[start_i-1]\n",
    "            start_i_qa += delta+1\n",
    "            start_i_qb += delta\n",
    "            qs = np.append(qs, q1[start_i_qa-1])\n",
    "        elif gap_indA == len(align.seqA):\n",
    "            break\n",
    "        else:\n",
    "            seq += align.seqA[start_i-1]\n",
    "            print('Double gap found')\n",
    "\n",
    "        # Final gap of the sequence B\n",
    "        if set(align.seqA[start_i:]) == {'-'}:\n",
    "            seq += align.seqB[start_i:]\n",
    "            qs = np.append(qs, q2[start_i_qb:])\n",
    "            #print(start_i, len(align.seqB))\n",
    "            break\n",
    "            \n",
    "    return seq, qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_seq(reads1, reads2, quals1, quals2, gap_penalty=-20):\n",
    "    seqs = []\n",
    "    av_qs = []\n",
    "    align_errors = []\n",
    "    align_scores = []\n",
    "    #index_map = []\n",
    "\n",
    "    for i in range(len(reads1)):\n",
    "        read1, read2 = reads1[i], Seq.reverse_complement(reads2[i])\n",
    "        qual1, qual2 = quals1[i], quals2[i][::-1]\n",
    "        if set(read1) != {'A','C','G','T'} or set(read2) != {'A','C','G','T'}:\n",
    "            align_errors.append(i)\n",
    "            continue\n",
    "            \n",
    "        alignment = pairwise2.align.localms(read1, read2, 1, -1, gap_penalty, gap_penalty)[0]\n",
    "        new_seq, new_qs = join_pairs(alignment, qual1, qual2)\n",
    "        if new_seq == []:\n",
    "            align_errors.append(i)\n",
    "        else:\n",
    "            seqs.append(new_seq)\n",
    "            av_qs.append(new_qs.mean())\n",
    "            #index_map.append(i)\n",
    "            align_scores.append(alignment.score/(alignment.end-alignment.start))\n",
    "            \n",
    "    return (*filter_seqs(seqs, av_qs, align_scores), len(align_errors))\n",
    "\n",
    "\n",
    "def filter_seqs(seqs, av_qs, align_scores):\n",
    "    # Discarding for quality overlap and length window\n",
    "    to_keep_q, to_keep_s, to_keep_l = set(), set(), set()\n",
    "    for i in range(len(av_qs)):\n",
    "        if av_qs[i] > q_threshold:\n",
    "            to_keep_q.add(i)\n",
    "        if align_scores[i] > s_threshold:\n",
    "            to_keep_s.add(i)\n",
    "        if len(seqs[i]) > len_bounds[0] and len(seqs[i]) < len_bounds[1]:\n",
    "            to_keep_l.add(i)\n",
    "    filt_seqs = np.take(seqs, list(to_keep_s.intersection(to_keep_q).intersection(to_keep_l)))\n",
    "\n",
    "    # Discarding for singletons\n",
    "    uniq_seqs, index, counts = np.unique(filt_seqs, return_counts=True, return_inverse=True)\n",
    "    to_keep_sing = set()\n",
    "    for i in range(len(index)):\n",
    "        if counts[index[i]] > 1:\n",
    "            to_keep_sing.add(i)\n",
    "    final_seqs = np.take(filt_seqs, list(to_keep_sing))\n",
    "    \n",
    "    return final_seqs, len(seqs)-len(to_keep_l), len(seqs)-len(to_keep_q), len(seqs)-len(to_keep_s), len(filt_seqs)-len(to_keep_sing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_and_export_seqs(seqs, name, directory):\n",
    "    uniq_seqs, counts = np.unique(seqs, return_counts=True)\n",
    "    f = open(directory+name+'.fasta', 'w')\n",
    "    for i in range(len(counts)):\n",
    "        f.write('>'+str(i+1)+'_'+str(counts[i])+'\\n')\n",
    "        f.write(uniq_seqs[i]+'\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gap_penalty = -20\n",
    "q_threshold = 32\n",
    "s_threshold = 0.66\n",
    "len_bounds = [400, 425]\n",
    "#good_lengths = [402, 405, 408, 411, 414, 417]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pat3_day2092\n",
      "pat9_day1365\n",
      "pat8_day1199\n",
      "pat9_day1149\n",
      "pat3_day436\n",
      "pat1_day4903\n",
      "pat1_day1474\n",
      "pat1_day4015\n",
      "pat4_day1266\n",
      "pat9_day1281\n",
      "pat2_day2542\n",
      "pat1_day3301\n",
      "pat7_day584\n",
      "pat2_day2170\n",
      "pat8_day72\n",
      "pat9_day687\n",
      "pat2_day4173\n",
      "pat2_day1646\n",
      "pat5_day1051\n",
      "pat1_day2468\n",
      "pat9_day778\n",
      "pat6_day2808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrea/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/andrea/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/andrea/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:234: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  keepdims=keepdims)\n",
      "/home/andrea/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:195: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/home/andrea/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:226: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pat1_day4498\n",
      "pat4_day2275\n",
      "pat1_day2314\n",
      "pat4_day1615\n",
      "pat2_day1002\n",
      "pat4_day911\n",
      "pat2_day1148\n",
      "pat9_day862\n",
      "pat1_day1803\n",
      "pat7_day1093\n",
      "pat7_day402\n",
      "pat1_day1313\n",
      "pat6_day1107\n",
      "pat3_day268\n",
      "pat2_day2700\n",
      "pat2_day3834\n",
      "pat6_day192\n",
      "pat9_day449\n",
      "pat2_day2829\n",
      "pat8_day440\n",
      "pat2_day1827\n",
      "pat2_day1469\n",
      "pat1_day3830\n",
      "pat3_day1777\n",
      "pat8_day196\n",
      "pat5_day275\n",
      "pat1_day2838\n",
      "pat7_day948\n",
      "pat5_day521\n",
      "pat6_day434\n",
      "pat7_day150\n",
      "pat1_day3669\n",
      "pat2_day3395\n",
      "pat2_day4656\n",
      "pat3_day1441\n",
      "pat6_day773\n",
      "pat8_day771\n",
      "pat6_day3122\n",
      "pat3_day97\n",
      "pat3_day2624\n",
      "pat8_day1112\n",
      "pat7_day50\n",
      "pat7_day1884\n",
      "pat4_day175\n",
      "pat7_day772\n",
      "pat7_day2438\n",
      "pat4_day764\n",
      "pat1_day4183\n",
      "pat10_day1229\n",
      "pat2_day4002\n",
      "pat5_day696\n",
      "pat1_day1961\n",
      "pat6_day2457\n",
      "pat3_day742\n",
      "pat1_day1646\n",
      "pat5_day1324\n",
      "pat7_day1261\n",
      "pat9_day1022\n",
      "pat8_day961\n",
      "pat6_day1785\n",
      "pat4_day427\n",
      "pat9_day79\n",
      "pat1_day3207\n",
      "pat4_day1954\n",
      "pat2_day2997\n",
      "pat2_day3169\n",
      "pat4_day1777\n",
      "pat3_day945\n",
      "pat9_day189\n",
      "pat1_day1144\n",
      "pat10_day1397\n",
      "pat5_day1205\n",
      "pat6_day2114\n",
      "pat1_day4660\n",
      "pat8_day1644\n",
      "pat8_day1359\n",
      "pat1_day3516\n",
      "pat5_day863\n",
      "pat5_day191\n",
      "pat6_day1448\n",
      "pat3_day1267\n",
      "pat2_day1989\n",
      "pat5_day361\n",
      "pat8_day603\n"
     ]
    }
   ],
   "source": [
    "info = pd.DataFrame(columns=['id', 'N_seqs', 'N_discarderd_singl', 'N_discarderd_len', 'N_discarded_quality', \\\n",
    "                             'N_discarded_align', 'N_discarded_overl', 'av_gene_len', \\\n",
    "                             'std_gene_len', 'overl_threshold'])\n",
    "\n",
    "for fastq in os.listdir('fastq/hiv/'):\n",
    "    \n",
    "    if fastq.split('_')[-1] == '1.fastq':\n",
    "        id_ = fastq.split('_')[0]+'_'+fastq.split('_')[1]\n",
    "        print(id_)\n",
    "        \n",
    "        if id_+'_hiv.fasta' in os.listdir('hiv_seqs/'):\n",
    "            print('already present, skip.')\n",
    "            continue\n",
    "        \n",
    "        name1, name2 = id_+'_hiv_1.fastq', id_+'_hiv_2.fastq'\n",
    "        reads1, reads2, quals1, quals2 = import_pair_reads(name1, name2)\n",
    "        seqs, l_fail, q_fail, overl_fail, sing_fail, a_fail = build_seq(reads1, reads2, quals1, quals2)\n",
    "        \n",
    "        info_d = {'id' : id_}\n",
    "        info_d['N_seqs']= len(seqs)\n",
    "        info_d['N_discarderd_len'] = l_fail\n",
    "        info_d['N_discarderd_singl'] = sing_fail\n",
    "        info_d['N_discarded_overl'] = overl_fail\n",
    "        info_d['N_discarded_align'] = a_fail\n",
    "        info_d['N_discarded_quality'] = q_fail\n",
    "        lens = [len(s) for s in seqs]\n",
    "        info_d['av_gene_len'] = np.mean(lens)\n",
    "        info_d['std_gene_len'] = np.std(lens)\n",
    "        info = info.append(info_d, ignore_index=True)\n",
    "        \n",
    "        group_and_export_seqs(seqs, id_+'_hiv', 'hiv_seqs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "info.to_csv('hiv_info/hiv_seqs_info.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_info = pd.read_csv('hiv_info/hiv_seqs_info.tsv', sep='\\t')\n",
    "old_info = old_info.set_index('id').drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = info.loc[set(info.index).difference(set(old_info.index))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = old_info.loc[['pat6_day2808', 'pat2_day3169']]\n",
    "info = info.rename(index={'pat6_day2808':'bubu'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = info.loc[set(info.index).difference(set(old_info.index))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(107, 106)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pd.concat([info, old_info])), len(old_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils as ut\n",
    "\n",
    "seqs = ut.import_fasta('hiv_seqs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['4_175', '8_1359', '3_1777', '4_764', '5_191', '2_2700', '1_3830', '4_2275', '2_2829', '6_773', '7_772', '2_1827', '5_863', '10_1229', '5_1205', '9_687', '5_1324', '7_1261', '7_50', '1_1961', '3_97', '1_4498', '1_3301', '8_1644', '2_2542', '2_1469', '1_1646', '6_2114', '2_4173', '3_2092', '7_402', '7_1884', '5_1051', '1_2838', '1_1144', '9_1149', '1_3516', '8_603', '9_189', '2_2997', '3_742', '3_2624', '5_521', '8_72', '2_3169', '9_1022', '2_2170', '3_945', '8_196', '8_1199', '6_2808', '1_4183', '3_1267', '4_1266', '3_1441', '1_4015', '7_2438', '2_1002', '1_3669', '2_3834', '1_2468', '7_150', '9_449', '9_79', '5_696', '1_1474', '7_1093', '6_3122', '6_1785', '1_4903', '8_961', '1_1803', '8_771', '3_268', '9_1365', '2_3395', '6_1448', '6_2457', '10_1397', '2_1989', '1_3207', '8_1112', '4_911', '6_192', '2_1148', '1_1313', '9_1281', '4_1777', '7_584', '9_862', '1_4660', '9_778', '2_1646', '5_361', '8_440', '4_1954', '5_275', '3_436', '6_434', '7_948', '4_1615', '2_4002', '4_427', '6_1107', '1_2314'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "exe = seqs['1_1144']\n",
    "stop_codons = {'TAG', 'TAA', 'TGA'}\n",
    "stop_codons_ar = [np.array(list(c)) for c in stop_codons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = list(exe.values())[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_bounds = [\n",
    "    [[0,0], [1,-2], [2,-1]], # Bounds of the 3 frames for length % 3 = 0\n",
    "    [[0,-1], [1,0], [2,-2]], # Bounds of the 3 frames for length % 3 = 1\n",
    "    [[0,-2], [1,-1], [2,0]], # Bounds of the 3 frames for length % 3 = 2\n",
    "]\n",
    "\n",
    "def get_n_codons_3frames(arr_seq, arr_codon):\n",
    "    l = len(arr_seq)\n",
    "    mod3 = l%3\n",
    "    n_codons = []\n",
    "    for fr in range(3): # Iter over the three frames\n",
    "        bound = resize_bounds[mod3][fr]\n",
    "        print(l, fr, bound)\n",
    "        tri_seq = arr_seq[bound[0]:l+bound[1]]\n",
    "        tri_seq = tri_seq.reshape((int(len(tri_seq)/3), 3))\n",
    "        n_codons.append(np.sum(np.all(tri_seq == arr_codon, axis=1)))\n",
    "    return n_codons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "411 0 [0, 0]\n",
      "411 1 [1, -2]\n",
      "411 2 [2, -1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 1, 0]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_n_codons_3frames(np.array(list(l)), stop_codons_ar[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
